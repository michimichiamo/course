<FrameworkSwitchCourse {fw} />

# Introduzione

Nel [Capitolo 3](/course/chapter3), abbiamo visto come affinare un modello per la classificazione testuale. In questo capitolo, affronteremo alcune operazioni molto comuni nell'ambito dell'elaborazione del linguaggio naturale (abbreviato NLP, dall'inglese *Natural Language Processing*):

- Classificazione dei token
- Masked language modeling (es. BERT)
- Sintetizzazione
- Traduzione
- Pre-addestramento di modelli causali del linguaggio (come GPT-2)
- Question answering

{#if fw === 'pt'}

Per farlo, dovremo avvalerci di tutto quello che abbiamo imparato riguardo all'API `Trainer` e alla libreria ðŸ¤— Accelerate nel [Capitolo 3](/course/chapter3), alla libreria ðŸ¤— Datasets nel [Capitolo 5](/course/chapter5), e alla libreria ðŸ¤— Tokenizers nel [Capitolo 6](/course/chapter6). Caricheremo inoltre i nostri risultati sul Model Hub, come abbiamo fatto nel [Capitolo 4](/course/chapter4), quindi in questo capitolo vedremo riuniti gli argomenti di tutti i capitoli!

Ogni sezione puÃ² essere consultata singolarmente e presenta come addestrare un modello con l'API `Trainer` API oppure con un ciclo di addestramento personalizzato, facendo uso della libreria ðŸ¤— Accelerate. Ogni parte puÃ² essere saltata, in modo da concentrarsi su quelle di maggiore interesse: l'API `Trainer` Ã¨ molto utile per affinare o addestrare un modello senza preoccuparsi di quello che succede dietro le quinte, mentre impostare un ciclo di addestramento con `Accelerate` ne semplifica la personalizzazione.

{:else}

Per farlo, dovremo avvalerci di tutto quello che abbiamo imparato riguardo all'addestramento di modelli con l'API di Keras nel [Capitolo 3](/course/chapter3), alla libreria ðŸ¤— Datasets nel [Capitolo 5](/course/chapter5), e alla libreria ðŸ¤— Tokenizers nel [Capitolo 6](/course/chapter6). Caricheremo inoltre i nostri risultati sul Model Hub, come abbiamo fatto nel [Capitolo 4](/course/chapter4), quindi in questo capitolo vedremo riuniti gli argomenti di tutti i capitoli!

Ogni sezione puÃ² essere consultata singolarmente.

{/if}


<Tip>

Leggendo il capitolo in modo sequenziale, si puÃ² notare come una parte significativa di codice e di testo sia condivisa tra le sezioni: la ripetizione Ã¨ intenzionale, ed Ã¨ volta a permettere l'approfondimento (o il ripasso) di ogni compito separatamente, fornendo un esempio autonomo, completo e funzionante.

</Tip>
